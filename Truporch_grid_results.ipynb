{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9e8d71-21d7-4514-8415-09fc8eb176f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RAG Quality Configuration Grid Test\n",
      "==================================================\n",
      "üß™ Starting Quality-Focused Grid Test\n",
      "==================================================\n",
      "üöÄ Setting up RAG system...\n",
      "‚úÖ RAG system ready\n",
      "üîß Generating parameter combinations...\n",
      "Total possible combinations: 65536\n",
      "Limiting to 12 random combinations\n",
      "‚úÖ Generated 12 configurations\n",
      "\n",
      "üèÉ Running 12 configurations...\n",
      "üìù Testing 5 queries per config\n",
      "üéØ Quality Metrics: Relevance, Accuracy, Coherence, Source Usage, Hallucination Resistance\n",
      "--------------------------------------------------\n",
      "Testing config 1... ‚úÖ Quality: 0.775, Relevance: 0.994\n",
      "Testing config 2... ‚úÖ Quality: 0.812, Relevance: 1.000\n",
      "Testing config 3... ‚úÖ Quality: 0.812, Relevance: 0.974\n",
      "Testing config 4... ‚úÖ Quality: 0.782, Relevance: 0.994\n",
      "Testing config 5... ‚úÖ Quality: 0.825, Relevance: 1.000\n",
      "Testing config 6... ‚úÖ Quality: 0.777, Relevance: 1.000\n",
      "Testing config 7... ‚úÖ Quality: 0.805, Relevance: 0.994\n",
      "Testing config 8... ‚úÖ Quality: 0.774, Relevance: 0.994\n",
      "Testing config 9... ‚úÖ Quality: 0.797, Relevance: 0.994\n",
      "Testing config 10... ‚úÖ Quality: 0.803, Relevance: 0.994\n",
      "Testing config 11... ‚úÖ Quality: 0.772, Relevance: 1.000\n",
      "Testing config 12... ‚úÖ Quality: 0.777, Relevance: 1.000\n",
      "\n",
      "üìä QUALITY ANALYSIS\n",
      "==================================================\n",
      "üî¢ Tested 12 configurations\n",
      "üéØ Average overall quality: 0.793\n",
      "üìä Average relevance: 0.995\n",
      "‚úÖ Average factual accuracy: 0.873\n",
      "üìù Average coherence: 0.572\n",
      "üìö Average source usage: 0.467\n",
      "üõ°Ô∏è  Average hallucination resistance: 0.940\n",
      "üìè Average response length: 254.3 words\n",
      "\n",
      "üèÜ TOP 5 HIGHEST OVERALL QUALITY:\n",
      " config_id  overall_quality  avg_relevance  avg_factual_accuracy  avg_coherence  temperature  top_p\n",
      "         5         0.825133       1.000000                  0.86       0.765667          0.3   0.95\n",
      "         3         0.811856       0.974286                  0.90       0.576424          0.1   0.70\n",
      "         2         0.811707       1.000000                  0.84       0.708535          0.5   0.90\n",
      "         7         0.804756       0.994286                  0.88       0.510923          0.1   0.95\n",
      "        10         0.803390       0.994286                  0.86       0.649091          0.7   0.95\n",
      "\n",
      "üéØ TOP 5 MOST RELEVANT:\n",
      " config_id  avg_relevance  overall_quality  temperature  retrieval_top_k  min_similarity\n",
      "         2            1.0         0.811707          0.5                2            0.30\n",
      "         5            1.0         0.825133          0.3                8            0.35\n",
      "         6            1.0         0.776566          0.5                2            0.25\n",
      "        11            1.0         0.772087          0.3                8            0.20\n",
      "        12            1.0         0.776724          0.1                2            0.35\n",
      "\n",
      "‚úÖ TOP 5 MOST FACTUALLY ACCURATE:\n",
      " config_id  avg_factual_accuracy  overall_quality  temperature  mirostat_eta  mirostat_tau\n",
      "         3                  0.90         0.811856          0.1           0.3           5.0\n",
      "         8                  0.90         0.773847          0.7           0.8          10.0\n",
      "         9                  0.90         0.796632          0.1           0.5           3.0\n",
      "         4                  0.88         0.782328          0.5           0.8           5.0\n",
      "         6                  0.88         0.776566          0.5           0.5           7.0\n",
      "\n",
      "üìö TOP 5 BEST SOURCE USAGE:\n",
      " config_id  avg_source_usage  overall_quality  retrieval_top_k  min_similarity  max_tokens\n",
      "         7              0.56         0.804756                8            0.25         500\n",
      "         3              0.52         0.811856                8            0.30         300\n",
      "         9              0.52         0.796632                8            0.30         500\n",
      "        11              0.52         0.772087                8            0.20         800\n",
      "         6              0.48         0.776566                2            0.25         500\n",
      "\n",
      "üìà PARAMETER CORRELATIONS WITH OVERALL QUALITY:\n",
      "  max_tokens: 0.533\n",
      "  temperature: 0.306\n",
      "  retrieval_top_k: 0.299\n",
      "  min_similarity: 0.245\n",
      "  top_p: 0.108\n",
      "  mirostat_eta: 0.105\n",
      "  mirostat_tau: 0.101\n",
      "\n",
      "üéâ Quality grid test completed!\n",
      "\n",
      "üìÅ Results saved to: quality_grid_results/quality_grid_test_1753216913_final.json\n",
      "üîç Check the CSV file for detailed quality metrics\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Import your refactored RAG system\n",
    "from rag_script3 import create_rag_instance, DEFAULT_CONFIG\n",
    "\n",
    "# TEST CONFIGURATIONS\n",
    "PARAMETER_GRID = {\n",
    "    \"temperature\": [0.1, 0.3, 0.5, 0.7],\n",
    "    \"top_p\": [0.7, 0.8, 0.9, 0.95],\n",
    "    \"top_k_llm\": [5, 10, 20, 40],  # LLM top_k\n",
    "    \"top_k_retrieval\": [2, 3, 5, 8],  # Retrieval top_k\n",
    "    \"min_similarity\": [0.2, 0.25, 0.3, 0.35],\n",
    "    \"max_tokens\": [200, 300, 500, 800],\n",
    "    \"mirostat_eta\": [0.1, 0.3, 0.5, 0.8],  # Learning rate for Mirostat algorithm\n",
    "    \"mirostat_tau\": [3.0, 5.0, 7.0, 10.0]  # Target entropy for Mirostat algorithm\n",
    "}\n",
    "\n",
    "# TEST QUERIES - Add more diverse queries for better testing\n",
    "TEST_QUERIES = [\n",
    "    \"What are the key factors to consider when analyzing a potential real estate investment?\",\n",
    "    \"How do I calculate cap rates for rental properties?\",\n",
    "    \"What are the risks of investing in commercial real estate?\",\n",
    "    \"Explain the difference between gross and net rental yields\",\n",
    "    \"What is the 1% rule in real estate investing?\"\n",
    "]\n",
    "\n",
    "class QualityEvaluator:\n",
    "    \"\"\"Evaluate response quality across multiple dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Keywords that might indicate hallucination or uncertainty\n",
    "        self.uncertainty_indicators = [\n",
    "            \"i don't know\", \"i'm not sure\", \"uncertain\", \"unclear\",\n",
    "            \"might be\", \"could be\", \"possibly\", \"perhaps\", \"maybe\"\n",
    "        ]\n",
    "        \n",
    "        # Keywords that indicate good source usage\n",
    "        self.source_indicators = [\n",
    "            \"according to\", \"based on\", \"as mentioned\", \"the document states\",\n",
    "            \"from the source\", \"referenced\", \"cited\", \"as shown in\"\n",
    "        ]\n",
    "    \n",
    "    def evaluate_relevance(self, query: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how relevant the answer is to the query.\n",
    "        Uses keyword overlap and topic alignment.\n",
    "        \"\"\"\n",
    "        # Extract key terms from query\n",
    "        query_words = set(re.findall(r'\\w+', query.lower()))\n",
    "        answer_words = set(re.findall(r'\\w+', answer.lower()))\n",
    "        \n",
    "        # Remove common stop words\n",
    "        stop_words = {\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\", \"by\", \"from\", \"up\", \"about\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"could\", \"should\", \"may\", \"might\", \"must\", \"shall\", \"can\"}\n",
    "        query_words -= stop_words\n",
    "        answer_words -= stop_words\n",
    "        \n",
    "        if not query_words:\n",
    "            return 0.5  # Neutral if no meaningful query words\n",
    "        \n",
    "        # Calculate overlap ratio\n",
    "        overlap = len(query_words.intersection(answer_words))\n",
    "        relevance_score = min(overlap / len(query_words), 1.0)\n",
    "        \n",
    "        # Bonus for topic-specific terms based on real estate context\n",
    "        real_estate_terms = {\"real estate\", \"property\", \"investment\", \"cap rate\", \"rental\", \"yield\", \"roi\", \"cash flow\", \"market\", \"valuation\", \"appreciation\"}\n",
    "        re_overlap = len(real_estate_terms.intersection(answer_words))\n",
    "        if re_overlap > 0:\n",
    "            relevance_score = min(relevance_score + 0.1 * re_overlap, 1.0)\n",
    "        \n",
    "        return relevance_score\n",
    "    \n",
    "    def evaluate_factual_accuracy(self, answer: str, sources: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate factual accuracy based on confidence indicators and source alignment.\n",
    "        This is a heuristic approach - true accuracy would need ground truth.\n",
    "        \"\"\"\n",
    "        answer_lower = answer.lower()\n",
    "        \n",
    "        # Penalty for uncertainty indicators\n",
    "        uncertainty_count = sum(1 for indicator in self.uncertainty_indicators if indicator in answer_lower)\n",
    "        uncertainty_penalty = min(uncertainty_count * 0.1, 0.3)\n",
    "        \n",
    "        # Bonus for specific numbers/facts (indicates concrete information)\n",
    "        number_matches = len(re.findall(r'\\b\\d+\\.?\\d*%?\\b', answer))\n",
    "        specificity_bonus = min(number_matches * 0.05, 0.2)\n",
    "        \n",
    "        # Base score starts neutral\n",
    "        accuracy_score = 0.7 - uncertainty_penalty + specificity_bonus\n",
    "        \n",
    "        return max(0.0, min(accuracy_score, 1.0))\n",
    "    \n",
    "    def evaluate_coherence(self, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate readability and coherence of the response.\n",
    "        \"\"\"\n",
    "        if not answer or len(answer.strip()) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        sentences = re.split(r'[.!?]+', answer)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            return 0.0\n",
    "        \n",
    "        # Average sentence length (ideal range: 15-25 words)\n",
    "        avg_sentence_length = np.mean([len(s.split()) for s in sentences])\n",
    "        length_score = 1.0 - abs(avg_sentence_length - 20) / 20\n",
    "        length_score = max(0.3, min(length_score, 1.0))\n",
    "        \n",
    "        # Sentence count (too few or too many can hurt coherence)\n",
    "        sentence_count = len(sentences)\n",
    "        if sentence_count < 2:\n",
    "            count_penalty = 0.2\n",
    "        elif sentence_count > 10:\n",
    "            count_penalty = 0.1\n",
    "        else:\n",
    "            count_penalty = 0.0\n",
    "        \n",
    "        # Check for repetitive patterns\n",
    "        words = answer.lower().split()\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if len(word) > 4:  # Only consider longer words\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Penalty for excessive repetition\n",
    "        max_freq = max(word_freq.values()) if word_freq else 1\n",
    "        repetition_penalty = min((max_freq - 3) * 0.05, 0.2) if max_freq > 3 else 0\n",
    "        \n",
    "        coherence_score = length_score - count_penalty - repetition_penalty\n",
    "        return max(0.0, min(coherence_score, 1.0))\n",
    "    \n",
    "    def evaluate_source_usage(self, answer: str, sources: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how well the response uses retrieved sources.\n",
    "        \"\"\"\n",
    "        if not sources:\n",
    "            return 0.0  # No sources available\n",
    "        \n",
    "        answer_lower = answer.lower()\n",
    "        \n",
    "        # Check for source integration indicators\n",
    "        source_integration = sum(1 for indicator in self.source_indicators if indicator in answer_lower)\n",
    "        integration_score = min(source_integration * 0.2, 0.6)\n",
    "        \n",
    "        # Length suggests more detailed use of sources\n",
    "        answer_length = len(answer.split())\n",
    "        length_factor = min(answer_length / 100, 1.0) * 0.3  # Up to 0.3 bonus for longer answers\n",
    "        \n",
    "        # Base score for having sources\n",
    "        base_score = 0.1\n",
    "        \n",
    "        source_score = base_score + integration_score + length_factor\n",
    "        return min(source_score, 1.0)\n",
    "    \n",
    "    def detect_hallucination(self, answer: str, query: str, sources: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Detect potential hallucinations (lower score = more hallucination detected).\n",
    "        This is heuristic-based.\n",
    "        \"\"\"\n",
    "        answer_lower = answer.lower()\n",
    "        \n",
    "        # Red flags for hallucination\n",
    "        red_flags = [\n",
    "            \"i remember\", \"i recall\", \"i know from experience\", \"i've seen\",\n",
    "            \"in my experience\", \"personally\", \"i believe\", \"i think\",\n",
    "            \"as far as i know\", \"from what i understand\"\n",
    "        ]\n",
    "        \n",
    "        hallucination_flags = sum(1 for flag in red_flags if flag in answer_lower)\n",
    "        \n",
    "        # Very specific claims without source attribution might be hallucinations\n",
    "        specific_claims = len(re.findall(r'\\b\\d{4}\\b', answer))  # Years\n",
    "        specific_claims += len(re.findall(r'\\$[\\d,]+', answer))  # Dollar amounts\n",
    "        specific_claims += len(re.findall(r'\\b\\d+\\.?\\d*%\\b', answer))  # Percentages\n",
    "        \n",
    "        # If many specific claims but no source indicators, potential hallucination\n",
    "        source_attribution = sum(1 for indicator in self.source_indicators if indicator in answer_lower)\n",
    "        \n",
    "        if specific_claims > 2 and source_attribution == 0:\n",
    "            specificity_penalty = 0.3\n",
    "        else:\n",
    "            specificity_penalty = 0.0\n",
    "        \n",
    "        # Score (1.0 = no hallucination detected, 0.0 = high hallucination)\n",
    "        hallucination_score = 1.0 - (hallucination_flags * 0.2) - specificity_penalty\n",
    "        return max(0.0, min(hallucination_score, 1.0))\n",
    "    \n",
    "    def evaluate_response(self, query: str, answer: str, sources: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate all quality dimensions.\"\"\"\n",
    "        return {\n",
    "            \"relevance\": self.evaluate_relevance(query, answer),\n",
    "            \"factual_accuracy\": self.evaluate_factual_accuracy(answer, sources),\n",
    "            \"coherence\": self.evaluate_coherence(answer),\n",
    "            \"source_usage\": self.evaluate_source_usage(answer, sources),\n",
    "            \"hallucination_resistance\": self.detect_hallucination(answer, query, sources),\n",
    "            \"response_length\": len(answer.split())\n",
    "        }\n",
    "\n",
    "class QualityGridTester:\n",
    "    \"\"\"Grid testing focused on response quality metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"quality_grid_results\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.results = []\n",
    "        self.rag_instance = None\n",
    "        self.evaluator = QualityEvaluator()\n",
    "        \n",
    "    def generate_configurations(self, max_configs: int = 50) -> List[Dict]:\n",
    "        \"\"\"Generate parameter combinations.\"\"\"\n",
    "        print(\"üîß Generating parameter combinations...\")\n",
    "        \n",
    "        # Get all combinations\n",
    "        keys = list(PARAMETER_GRID.keys())\n",
    "        values = list(PARAMETER_GRID.values())\n",
    "        all_combinations = list(itertools.product(*values))\n",
    "        \n",
    "        print(f\"Total possible combinations: {len(all_combinations)}\")\n",
    "        \n",
    "        # Limit if too many\n",
    "        if len(all_combinations) > max_configs:\n",
    "            print(f\"Limiting to {max_configs} random combinations\")\n",
    "            np.random.seed(42)  # Reproducible\n",
    "            indices = np.random.choice(len(all_combinations), max_configs, replace=False)\n",
    "            all_combinations = [all_combinations[i] for i in indices]\n",
    "        \n",
    "        # Convert to config dicts\n",
    "        configs = []\n",
    "        for combo in all_combinations:\n",
    "            config = DEFAULT_CONFIG.copy()\n",
    "            \n",
    "            # Map parameters to config structure\n",
    "            config[\"llm_options\"] = config[\"llm_options\"].copy()\n",
    "            config[\"llm_options\"][\"temperature\"] = combo[keys.index(\"temperature\")]\n",
    "            config[\"llm_options\"][\"top_p\"] = combo[keys.index(\"top_p\")]\n",
    "            config[\"llm_options\"][\"top_k\"] = combo[keys.index(\"top_k_llm\")]\n",
    "            config[\"llm_options\"][\"num_predict\"] = combo[keys.index(\"max_tokens\")]\n",
    "            config[\"llm_options\"][\"mirostat_eta\"] = combo[keys.index(\"mirostat_eta\")]\n",
    "            config[\"llm_options\"][\"mirostat_tau\"] = combo[keys.index(\"mirostat_tau\")]\n",
    "            \n",
    "            config[\"top_k\"] = combo[keys.index(\"top_k_retrieval\")]\n",
    "            config[\"min_similarity\"] = combo[keys.index(\"min_similarity\")]\n",
    "            \n",
    "            configs.append(config)\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(configs)} configurations\")\n",
    "        return configs\n",
    "    \n",
    "    def setup_rag(self) -> bool:\n",
    "        \"\"\"Setup RAG instance once.\"\"\"\n",
    "        print(\"üöÄ Setting up RAG system...\")\n",
    "        self.rag_instance = create_rag_instance()\n",
    "        self.rag_instance.set_verbose(False)  # Quiet mode for testing\n",
    "        \n",
    "        if not self.rag_instance.setup():\n",
    "            print(\"‚ùå RAG setup failed\")\n",
    "            return False\n",
    "        \n",
    "        print(\"‚úÖ RAG system ready\")\n",
    "        return True\n",
    "    \n",
    "    def test_configuration(self, config: Dict, config_id: int) -> Dict:\n",
    "        \"\"\"Test a single configuration.\"\"\"\n",
    "        print(f\"Testing config {config_id}... \", end=\"\")\n",
    "        \n",
    "        # Update RAG with new config\n",
    "        self.rag_instance.update_config(config)\n",
    "        self.rag_instance.reset_stats()\n",
    "        self.rag_instance.clear_cache()  # Fresh start for each config\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Test all queries\n",
    "        for query in TEST_QUERIES:\n",
    "            try:\n",
    "                result = self.rag_instance.ask(query, use_cache=False)\n",
    "                \n",
    "                # Extract source content for evaluation\n",
    "                source_texts = [source.get(\"content\", \"\") for source in result.get(\"sources\", [])]\n",
    "                \n",
    "                # Evaluate quality\n",
    "                quality_scores = self.evaluator.evaluate_response(\n",
    "                    query, \n",
    "                    result[\"answer\"], \n",
    "                    source_texts\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"answer\": result[\"answer\"],\n",
    "                    \"sources_count\": len(result.get(\"sources\", [])),\n",
    "                    \"quality_scores\": quality_scores,\n",
    "                    \"success\": result[\"error\"] is None\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"query\": query,\n",
    "                    \"answer\": f\"Exception: {str(e)}\",\n",
    "                    \"sources_count\": 0,\n",
    "                    \"quality_scores\": {\n",
    "                        \"relevance\": 0.0,\n",
    "                        \"factual_accuracy\": 0.0,\n",
    "                        \"coherence\": 0.0,\n",
    "                        \"source_usage\": 0.0,\n",
    "                        \"hallucination_resistance\": 0.0,\n",
    "                        \"response_length\": 0\n",
    "                    },\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        successful_results = [r for r in results if r[\"success\"]]\n",
    "        if successful_results:\n",
    "            avg_relevance = np.mean([r[\"quality_scores\"][\"relevance\"] for r in successful_results])\n",
    "            avg_factual_accuracy = np.mean([r[\"quality_scores\"][\"factual_accuracy\"] for r in successful_results])\n",
    "            avg_coherence = np.mean([r[\"quality_scores\"][\"coherence\"] for r in successful_results])\n",
    "            avg_source_usage = np.mean([r[\"quality_scores\"][\"source_usage\"] for r in successful_results])\n",
    "            avg_hallucination_resistance = np.mean([r[\"quality_scores\"][\"hallucination_resistance\"] for r in successful_results])\n",
    "            avg_response_length = np.mean([r[\"quality_scores\"][\"response_length\"] for r in successful_results])\n",
    "            \n",
    "            # Overall quality score (weighted average)\n",
    "            overall_quality = (\n",
    "                avg_relevance * 0.25 +\n",
    "                avg_factual_accuracy * 0.25 +\n",
    "                avg_coherence * 0.2 +\n",
    "                avg_source_usage * 0.15 +\n",
    "                avg_hallucination_resistance * 0.15\n",
    "            )\n",
    "        else:\n",
    "            avg_relevance = avg_factual_accuracy = avg_coherence = 0.0\n",
    "            avg_source_usage = avg_hallucination_resistance = avg_response_length = 0.0\n",
    "            overall_quality = 0.0\n",
    "        \n",
    "        config_result = {\n",
    "            \"config_id\": config_id,\n",
    "            \"config\": config,\n",
    "            \"quality_metrics\": {\n",
    "                \"avg_relevance\": avg_relevance,\n",
    "                \"avg_factual_accuracy\": avg_factual_accuracy,\n",
    "                \"avg_coherence\": avg_coherence,\n",
    "                \"avg_source_usage\": avg_source_usage,\n",
    "                \"avg_hallucination_resistance\": avg_hallucination_resistance,\n",
    "                \"avg_response_length\": avg_response_length,\n",
    "                \"overall_quality\": overall_quality,\n",
    "                \"successful_queries\": len(successful_results)\n",
    "            },\n",
    "            \"detailed_results\": results\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Quality: {overall_quality:.3f}, Relevance: {avg_relevance:.3f}\")\n",
    "        return config_result\n",
    "    \n",
    "    def run_grid_test(self, max_configs: int = 20) -> str:\n",
    "        \"\"\"Run full grid test.\"\"\"\n",
    "        print(\"üß™ Starting Quality-Focused Grid Test\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if not self.setup_rag():\n",
    "            return \"\"\n",
    "        \n",
    "        configs = self.generate_configurations(max_configs)\n",
    "        \n",
    "        print(f\"\\nüèÉ Running {len(configs)} configurations...\")\n",
    "        print(f\"üìù Testing {len(TEST_QUERIES)} queries per config\")\n",
    "        print(\"üéØ Quality Metrics: Relevance, Accuracy, Coherence, Source Usage, Hallucination Resistance\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for i, config in enumerate(configs, 1):\n",
    "            result = self.test_configuration(config, i)\n",
    "            self.results.append(result)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if i % 5 == 0:\n",
    "                self.save_results(f\"intermediate_{i}\")\n",
    "        \n",
    "        # Final save and analysis\n",
    "        results_file = self.save_results(\"final\")\n",
    "        self.analyze_results()\n",
    "        \n",
    "        print(\"\\nüéâ Quality grid test completed!\")\n",
    "        return results_file\n",
    "    \n",
    "    def save_results(self, suffix: str = \"\") -> str:\n",
    "        \"\"\"Save results to files.\"\"\"\n",
    "        timestamp = int(time.time())\n",
    "        base_name = f\"quality_grid_test_{timestamp}_{suffix}\"\n",
    "        \n",
    "        # Save raw results\n",
    "        json_file = self.output_dir / f\"{base_name}.json\"\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        # Save metrics summary\n",
    "        metrics_data = []\n",
    "        for result in self.results:\n",
    "            row = {\"config_id\": result[\"config_id\"]}\n",
    "            row.update(result[\"quality_metrics\"])\n",
    "            \n",
    "            # Add key config parameters\n",
    "            config = result[\"config\"]\n",
    "            row.update({\n",
    "                \"temperature\": config[\"llm_options\"][\"temperature\"],\n",
    "                \"top_p\": config[\"llm_options\"][\"top_p\"],\n",
    "                \"llm_top_k\": config[\"llm_options\"][\"top_k\"],\n",
    "                \"retrieval_top_k\": config[\"top_k\"],\n",
    "                \"min_similarity\": config[\"min_similarity\"],\n",
    "                \"max_tokens\": config[\"llm_options\"][\"num_predict\"],\n",
    "                \"mirostat_eta\": config[\"llm_options\"][\"mirostat_eta\"],\n",
    "                \"mirostat_tau\": config[\"llm_options\"][\"mirostat_tau\"]\n",
    "            })\n",
    "            \n",
    "            metrics_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        csv_file = self.output_dir / f\"{base_name}_metrics.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        return str(json_file)\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze and print results.\"\"\"\n",
    "        if not self.results:\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìä QUALITY ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Create metrics DataFrame\n",
    "        metrics_data = []\n",
    "        for result in self.results:\n",
    "            row = {\"config_id\": result[\"config_id\"]}\n",
    "            row.update(result[\"quality_metrics\"])\n",
    "            row.update({\n",
    "                \"temperature\": result[\"config\"][\"llm_options\"][\"temperature\"],\n",
    "                \"top_p\": result[\"config\"][\"llm_options\"][\"top_p\"],\n",
    "                \"llm_top_k\": result[\"config\"][\"llm_options\"][\"top_k\"],\n",
    "                \"retrieval_top_k\": result[\"config\"][\"top_k\"],\n",
    "                \"min_similarity\": result[\"config\"][\"min_similarity\"],\n",
    "                \"max_tokens\": result[\"config\"][\"llm_options\"][\"num_predict\"],\n",
    "                \"mirostat_eta\": result[\"config\"][\"llm_options\"][\"mirostat_eta\"],\n",
    "                \"mirostat_tau\": result[\"config\"][\"llm_options\"][\"mirostat_tau\"]\n",
    "            })\n",
    "            metrics_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"üî¢ Tested {len(self.results)} configurations\")\n",
    "        print(f\"üéØ Average overall quality: {df['overall_quality'].mean():.3f}\")\n",
    "        print(f\"üìä Average relevance: {df['avg_relevance'].mean():.3f}\")\n",
    "        print(f\"‚úÖ Average factual accuracy: {df['avg_factual_accuracy'].mean():.3f}\")\n",
    "        print(f\"üìù Average coherence: {df['avg_coherence'].mean():.3f}\")\n",
    "        print(f\"üìö Average source usage: {df['avg_source_usage'].mean():.3f}\")\n",
    "        print(f\"üõ°Ô∏è  Average hallucination resistance: {df['avg_hallucination_resistance'].mean():.3f}\")\n",
    "        print(f\"üìè Average response length: {df['avg_response_length'].mean():.1f} words\")\n",
    "        \n",
    "        # Best configurations\n",
    "        print(\"\\nüèÜ TOP 5 HIGHEST OVERALL QUALITY:\")\n",
    "        best_overall = df.nlargest(5, 'overall_quality')[['config_id', 'overall_quality', 'avg_relevance', 'avg_factual_accuracy', 'avg_coherence', 'temperature', 'top_p']]\n",
    "        print(best_overall.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüéØ TOP 5 MOST RELEVANT:\")\n",
    "        best_relevance = df.nlargest(5, 'avg_relevance')[['config_id', 'avg_relevance', 'overall_quality', 'temperature', 'retrieval_top_k', 'min_similarity']]\n",
    "        print(best_relevance.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n‚úÖ TOP 5 MOST FACTUALLY ACCURATE:\")\n",
    "        best_accuracy = df.nlargest(5, 'avg_factual_accuracy')[['config_id', 'avg_factual_accuracy', 'overall_quality', 'temperature', 'mirostat_eta', 'mirostat_tau']]\n",
    "        print(best_accuracy.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüìö TOP 5 BEST SOURCE USAGE:\")\n",
    "        best_sources = df.nlargest(5, 'avg_source_usage')[['config_id', 'avg_source_usage', 'overall_quality', 'retrieval_top_k', 'min_similarity', 'max_tokens']]\n",
    "        print(best_sources.to_string(index=False))\n",
    "        \n",
    "        # Parameter correlations\n",
    "        print(f\"\\nüìà PARAMETER CORRELATIONS WITH OVERALL QUALITY:\")\n",
    "        quality_correlations = df[['overall_quality', 'temperature', 'top_p', 'llm_top_k', 'retrieval_top_k', 'min_similarity', 'max_tokens', 'mirostat_eta', 'mirostat_tau']].corr()['overall_quality'].abs().sort_values(ascending=False)\n",
    "        for param, corr in quality_correlations.items():\n",
    "            if param != 'overall_quality' and abs(corr) > 0.1:\n",
    "                print(f\"  {param}: {corr:.3f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the quality-focused grid test.\"\"\"\n",
    "    print(\"üß™ RAG Quality Configuration Grid Test\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Configuration\n",
    "    max_configs = 12  # Adjust based on your time budget\n",
    "    \n",
    "    # Run test\n",
    "    tester = QualityGridTester()\n",
    "    results_file = tester.run_grid_test(max_configs)\n",
    "    \n",
    "    print(f\"\\nüìÅ Results saved to: {results_file}\")\n",
    "    print(\"üîç Check the CSV file for detailed quality metrics\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
